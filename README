### requirement
pytorch version >= 2.0 # to use flash attention

### datasets
- training datasets: openwebtext or fineweb-edu, both are supported
- evaluation dataset: hellaswag

### first, prepare the dataset
#### if you want to use openwebtext, running the following command:
```sh
python openwebtext_prepare.py
```
after running we get:

- train.bin is ~17GB, val.bin ~8.5MB
- train has ~9B tokens (9,035,582,198)
- val has ~4M tokens (4,434,897)

this came from 8,013,769 documents in total.
- [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) dataset

#### if you want to use fine-web-edu, running the following command:
```sh
python fineweb-edu.py
```
after running we get:

- the smallest sample version of fineweb-edu(10BT version)
- it is split to 100 shards, 100MT per shard

