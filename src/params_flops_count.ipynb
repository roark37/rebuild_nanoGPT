{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count the number of params/flops in GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Paramters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1 count from model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_args = {\n",
    "    'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "    'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "    'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "    'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "}['gpt2']\n",
    "\n",
    "class GPTConfigs():\n",
    "    def __init__(self, config_args):\n",
    "        self.n_layer = config_args['n_layer']\n",
    "        self.n_head = config_args['n_head']\n",
    "        self.n_embd = config_args['n_embd']\n",
    "        self.seq_length = 1024\n",
    "        self.vocab_size = 50257\n",
    "        self.bias = False         \n",
    "        assert not self.bias, 'Assumes False bias for simplicity.'\n",
    "\n",
    "nanoGPT_config = GPTConfigs(config_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params():\n",
    "    def __init__(self, config_args):\n",
    "        self.cfg = config_args\n",
    "\n",
    "    def params_count(self):\n",
    "        vocab_size = self.cfg.vocab_size\n",
    "        n_embd = self.cfg.n_embd\n",
    "        seq_length = self.cfg.seq_length\n",
    "        n_layer = self.cfg.n_layer\n",
    "        out = OrderedDict()\n",
    "\n",
    "        # token and position embeddings\n",
    "        out['embed_token'] = vocab_size * n_embd\n",
    "        out['embed_posi'] = seq_length * n_embd\n",
    "        out['embed'] = out['embed_token'] + out['embed_posi']\n",
    "\n",
    "        # an attention block\n",
    "        out['att_ln'] = n_embd # for gamma, no beta(bias is False in the nanoGPT)\n",
    "        out['att_kqv'] = n_embd * n_embd * 3\n",
    "        out['att_proj'] = n_embd * n_embd\n",
    "        out['att'] = out['att_ln'] + out['att_kqv'] + out['att_proj']\n",
    "\n",
    "        # a MLP block\n",
    "        ff_size = 4 * n_embd\n",
    "        out['mlp_ln'] = n_embd\n",
    "        out['mlp_ff'] = n_embd * ff_size\n",
    "        out['mlp_proj'] = ff_size * n_embd\n",
    "        out['mlp'] = out['mlp_ln'] + out['mlp_ff'] + out['mlp_proj']\n",
    "\n",
    "        # all key blocks\n",
    "        out['block'] = out['att'] + out['mlp']\n",
    "        out['total_blocks'] = n_layer * out['block']\n",
    "\n",
    "        # other: final layer norm and linear\n",
    "        out['ln_final'] = n_embd\n",
    "        out['linear_final'] = 0 # uncounted because of weight sharing\n",
    "\n",
    "        # total\n",
    "        out['total'] = out['embed'] + out['total_blocks'] + out['ln_final']\n",
    "        out['total_minus_embed'] = out['total'] - out['embed_posi']\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we see: 124337664, expected: 124337664, match: True\n"
     ]
    }
   ],
   "source": [
    "p = Params(nanoGPT_config).params_count()\n",
    "params_total = p['total']\n",
    "params_no_embd = p['total_minus_embed']\n",
    "print(f\"we see: {params_total}, expected: {124337664}, match: {params_total == 124337664}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_token                38.6M       31.0%\n",
      "embed_posi                  0.8M        0.6%\n",
      "embed                      39.4M       31.7%\n",
      "att_ln                      0.0M        0.0%\n",
      "att_kqv                     1.8M        1.4%\n",
      "att_proj                    0.6M        0.5%\n",
      "att                         2.4M        1.9%\n",
      "mlp_ln                      0.0M        0.0%\n",
      "mlp_ff                      2.4M        1.9%\n",
      "mlp_proj                    2.4M        1.9%\n",
      "mlp                         4.7M        3.8%\n",
      "block                       7.1M        5.7%\n",
      "total_blocks               85.0M       68.3%\n",
      "ln_final                    0.0M        0.0%\n",
      "linear_final                0.0M        0.0%\n",
      "total                     124.3M      100.0%\n",
      "total_minus_embed         123.6M       99.4%\n"
     ]
    }
   ],
   "source": [
    "for k,v in p.items():\n",
    "    print(f\"{k:20s} {v/1e6:10.1f}M {v/params_total*100:10.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2 count from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est checkpoint size: 1492051968bytes = 1.49GB\n"
     ]
    }
   ],
   "source": [
    "# params are stored in fp32 \n",
    "params_bytes = params_total * 4\n",
    "# the AdamW optimizer has 2 additional buffers per param for statistics\n",
    "param_opt_bytes = params_bytes + 2 * params_bytes\n",
    "\n",
    "print(f'est checkpoint size: {param_opt_bytes}bytes = {param_opt_bytes/1e9:.2f}GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**in the buffers of AdamW**\n",
    "- the AdamW optimizer has 2 additional buffers per param for statistics\n",
    "- in the checkpoint of nanoGPT file, the params can be referred to as: \n",
    "  ```python\n",
    "  ckpt = torch.load('../log/model_00050.pt')\n",
    "  states = ckpt1['optimizer']['state']  # all params are saved here\n",
    "  param0 = states[0]    # states of params can be referred by their order in the model\n",
    "  param40 = states[40]  # each param has two states for 1st and 2nd moment est\n",
    "  ```\n",
    "- each parameter has its key(a number) in the `states`, each `states` has two copy of the value of the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt.pt size: 1493898466bytes = 1.49GB\n"
     ]
    }
   ],
   "source": [
    "# word count the checkpoint file\n",
    "measured = !wc -c ../log/model_00050.pt \n",
    "measured_bytes = int((measured[0].split(' '))[0])\n",
    "print(f\"ckpt.pt size: {measured_bytes}bytes = {measured_bytes/1e9:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt.pt size: 497963882bytes = 0.50GB\n"
     ]
    }
   ],
   "source": [
    "# word count of a checkpoint file with an empty optimizer \n",
    "measured_no_opt = !wc -c ../log/model_00000.pt \n",
    "measured_no_opt_bytes = int((measured_no_opt[0].split(' '))[0])\n",
    "print(f\"ckpt.pt size: {measured_no_opt_bytes}bytes = {measured_no_opt_bytes/1e9:.2f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3 GPU memory occupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory taken up for parameters: 6.22%\n"
     ]
    }
   ],
   "source": [
    "mem_4090 = 24e9\n",
    "print(f\"memory taken up for parameters: {param_opt_bytes / mem_4090 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Flops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1 count from model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- only count Weight(Matrix Multiply) FLOPs, all others (LayerNorm, Softmax, etc) are effectively irrelevant.\n",
    "  - because the computational complexity of matrix multiplication is O(n²m) for an n×m matrix, which scales much faster than operations like LayerNorm O(n) or Softmax O(n)\n",
    "  - and Weight operations involve large matrix multiplications that can be highly optimized using parallel processing and specialized matrix multiplication units. but other operations often require more sequential processing and memory access\n",
    "  - For most practical purposes, weight FLOPs give a good approximation of relative computational cost between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flops():\n",
    "    def __init__(self, config_args):\n",
    "        self.cfg = config_args\n",
    "\n",
    "    def flops_count(self):\n",
    "        \"\"\"\n",
    "        count the flops for processing one sequence\n",
    "        count actual FLOPs, not MACs(multiple add). Hence 2* all over the place\n",
    "        matrix multiply X (BxC) @ Y (CxD) -> Z (BxD) flops are ~2*B*C*D\n",
    "        the flops is for one sequence of seq_length length.\n",
    "        \"\"\"\n",
    "\n",
    "        vocab_size = self.cfg.vocab_size\n",
    "        n_embd = self.cfg.n_embd\n",
    "        seq_length = self.cfg.seq_length\n",
    "        n_layer = self.cfg.n_layer\n",
    "        n_head = self.cfg.n_head        \n",
    "        head_size = n_embd // n_head\n",
    "        out = OrderedDict()\n",
    "\n",
    "        ## attention block\n",
    "        #  1) projection to K, Q, V\n",
    "        out['att_kqv'] = 2 * seq_length * (n_embd * 3 * n_embd)\n",
    "        #  2) QK scores\n",
    "        out['att_qkscores'] = 2 * seq_length * (seq_length * n_embd)\n",
    "        #  3) weighted average: (B, nh, T, T) @ (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        out['att_alpha'] = 2 * n_head * seq_length * (seq_length * head_size)\n",
    "        #  4) attention projection\n",
    "        out['att_proj'] = 2 * seq_length * (n_embd * n_embd)\n",
    "        out['att'] = sum(out['att_'+i] for i in ['kqv', 'qkscores', 'alpha', 'proj'])\n",
    "\n",
    "        ## MLP block\n",
    "        ff_size = 4 * n_embd\n",
    "        out['mlp_ff'] = 2 * seq_length * n_embd * ff_size\n",
    "        out['mlp_proj'] = 2 * seq_length * ff_size * n_embd\n",
    "        out['mlp'] = out['mlp_ff'] + out['mlp_proj']\n",
    "\n",
    "        ## all key blocks\n",
    "        out['block'] = out['att'] + out['mlp']\n",
    "        out['total_blocks'] = n_layer * out['block']\n",
    "\n",
    "        # other: final linear\n",
    "        out['linear_final'] = 2 * seq_length * n_embd * vocab_size\n",
    "\n",
    "        # total\n",
    "        out['forward_total'] = out['total_blocks'] + out['linear_final']\n",
    "        out['backward_total'] = 2 * out['forward_total']\n",
    "        out['total'] = out['forward_total'] + out['backward_total']\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                 flops          ratio (%) \n",
      "att_kqv              3624 M        1.2%\n",
      "att_qkscores         1611 M        0.6%\n",
      "att_alpha            1611 M        0.6%\n",
      "att_proj             1208 M        0.4%\n",
      "att                  8053 M        2.8%\n",
      "mlp_ff               4832 M        1.7%\n",
      "mlp_proj             4832 M        1.7%\n",
      "mlp                  9664 M        3.3%\n",
      "block                17717 M        6.1%\n",
      "total_blocks         212601 M       72.9%\n",
      "linear_final         79047 M       27.1%\n",
      "forward_total        291648 M      100.0%\n",
      "backward_total       583297 M      200.0%\n",
      "total                874945 M      300.0%\n"
     ]
    }
   ],
   "source": [
    "f = Flops(nanoGPT_config).flops_count()\n",
    "flops_forward = f['forward_total']\n",
    "print(f\"{'name':20s} {'flops':14s} {'ratio (%)':10s}\")\n",
    "for k,v in f.items():\n",
    "    print(f\"{k:20s} {v/1e6:.0f} M {v/flops_forward*100:10.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2 model flops utilization (MFU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4090:\n",
    "  - 4090_bf16_flops_promised = 165T   # with FP32 accumulate\n",
    "  - 4090_tf32_flops_promised = 82.6T\n",
    "  - 4090_fp32_flops_promised = 82.6T  # pytorch default\n",
    "- 3090:\n",
    "  - 3090_bf16_flops_promised = 71T    # with FP32 accumulate\n",
    "  - 3090_tf32_flops_promised = 35.6T\n",
    "  - 3090_fp32_flops_promised = 35.6T  # pytorch default\n",
    "- A100:\n",
    "  - A100_bf16_flops_promised = 312T   # with FP32 accumulate\n",
    "  - A100_tf32_flops_promised = 156T\n",
    "  - A100_fp32_flops_promised = 19.5T  # pytorch default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_bf16_cap = {\n",
    "    '3090': 71e12, # flops per second\n",
    "    '4090': 165e12\n",
    "}\n",
    "gpu_tf32_cap = {\n",
    "    '3090': 82.6e12,\n",
    "    '4090': 35.6e12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each sequence need 8.16 micro seconds\n"
     ]
    }
   ],
   "source": [
    "total_bsize=589824 # grad_accum_steps * (B * T), measured the #tokens in a macro batch\n",
    "B = 24\n",
    "T = 1024\n",
    "total_seq = total_bsize / 1024\n",
    "\n",
    "dt = 4700 # measured by ms \n",
    "dt_per_seq = dt / total_seq # measured by ms\n",
    "print(f'each sequence need {dt_per_seq:.2f} micro seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about 107.23 Tflops is done in one second\n"
     ]
    }
   ],
   "source": [
    "flops_per_seq = f['total']\n",
    "flops_per_second = flops_per_seq / dt_per_seq * 1000\n",
    "print(f'about {flops_per_second/1e12:.2f} Tflops is done in one second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MFU (model flops utilization) of nanoGPT is: 65.0%\n"
     ]
    }
   ],
   "source": [
    "flops_achieved = flops_per_second\n",
    "gpu_cap = gpu_bf16_cap['4090'] \n",
    "fraction_of_gpu_used = flops_achieved / gpu_cap\n",
    "print(f'The MFU (model flops utilization) of nanoGPT is: {fraction_of_gpu_used*100:.1f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3 total cost(flops) of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_time_cost(model_size, token_num, gpu, gpu_num, assumed_mfu):   \n",
    "    flops_needed = 6 * model_size * token_num # est in the scaling law paper\n",
    "\n",
    "    rtx4090_flops = gpu_bf16_cap[gpu] \n",
    "    flops_throughput = rtx4090_flops * assumed_mfu * gpu_num\n",
    "\n",
    "    time_needed_s = flops_needed / flops_throughput\n",
    "    return time_needed_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time needed to train the model: 31.2 days.\n"
     ]
    }
   ],
   "source": [
    "model_size = params_no_embd\n",
    "token_num = 300e9 # suppose 300B tokens\n",
    "gpu = '4090'\n",
    "gpu_num = 1\n",
    "assumed_mfu = 0.5\n",
    "\n",
    "seconds_cost = model_time_cost(model_size, token_num, gpu, gpu_num, assumed_mfu)\n",
    "print(f'time needed to train the model: {seconds_cost/3600/24:.1f} days.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
